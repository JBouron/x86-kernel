#include <test.h>

static bool volatile simple_test_message_received = false;
static uint8_t simple_test_sender_id = 0;
static void simple_test_callback(struct ipm_message const * const msg) {
    simple_test_message_received = msg->tag == __TEST &&
        msg->sender_id == simple_test_sender_id;
}

static bool ipm_simple_test(void) {
    simple_test_sender_id = cpu_id();
    TEST_TAG_CALLBACK = simple_test_callback;

    // Enable interrupts on the current cpu as it will send a message to itself.
    cpu_set_interrupt_flag(true);
    for (uint8_t cpu = 0; cpu < acpi_get_number_cpus(); ++cpu) {
        simple_test_message_received = false;
        send_ipm(cpu, __TEST, NULL, 0);

        uint8_t iter = 0;
        uint8_t const max_iters = 100;
        while (!simple_test_message_received) {
            lapic_sleep(10);
            ++iter;
            if (iter > max_iters) {
                return false;
            }
        }
    }
    return true;
}

// Remote call test:
// The cpu running the test send, to each core one by one (incl itself) a remote
// call (REMOTE_CALL) message. This message will trigger a call to
// remote_call_func. The argument is a unique value that the remote cpu must
// write to remote_call_flag.

static uint32_t remote_call_flag = 0;

static void remote_call_func(void * arg) {
    uint32_t const val = (uint32_t)arg;
    LOG("CPU %u exec remote call, value = %u\n", cpu_id(), val);
    remote_call_flag = val;
}

static bool ipm_remote_call_test(void) {
    uint8_t const ncpus = acpi_get_number_cpus();
    // STI since we will send a message to ourselves.
    cpu_set_interrupt_flag(true);

    for (uint8_t cpu = 0; cpu < ncpus; ++cpu) {
        // Read the TSC to get a somewhat unique value for each cpu.
        uint32_t const val = read_tsc() & 0xFFFFFFFFULL;

        // Make sure that the value of the remote call flag is set to something
        // different that `val`.
        remote_call_flag = ~val;

        exec_remote_call(cpu, remote_call_func, (void*)val, true);

        // We called exec_remote_call with wait = true, therefore upon returning
        // from this function, the following must be true.
        TEST_ASSERT(remote_call_flag == val);
    }

    return true;
}

// Overload test: Make all other cpus send a message to the current cpu
// concurrently. This is not only a stress test for the concurrency of the
// message queue but this also test that the target cpu will still receive
// message despite some IPI being dropped while the target is executing the
// interrupt handler (this will certainly happen in systems with a lot of cpus).
//
// The current cpu will make the other cpu send it __TEST messages through
// REMOTE_CALL messages.

// The number of message per core must not be too high, otherwise it could
// trigger a stack overflow on the receiving cpu.
static uint32_t overload_test_num_messages_per_core = 8;
static bool volatile overload_test_ready = false;
static bool volatile ipi_sent = false;

static void overload_test_wait_and_send_messages(void * arg) {
    uint32_t const dest = (uint32_t)arg;

    while (!overload_test_ready) {
        cpu_pause();
    }

    for (uint32_t i = 0; i < overload_test_num_messages_per_core; ++i) {
        send_ipm(dest, __TEST, NULL, 0);
        ipi_sent = true;
    }
}

static uint32_t *num_received_msg_per_core = NULL;
static void overload_test_message_callback(struct ipm_message const * msg) {
    // Need to disable interrupt here as the increment is not atomic and we
    // might receive another message in the middle of it.
    cpu_set_interrupt_flag(false);
    num_received_msg_per_core[msg->sender_id] ++;
    cpu_set_interrupt_flag(true);
}

static bool overload_test_all_messages_received(void) {
    uint8_t const ncpus = acpi_get_number_cpus();
    for (uint8_t cpu = 0; cpu < ncpus; ++cpu) {
        if (cpu == cpu_id()) {
            continue;
        } else if (num_received_msg_per_core[cpu] !=
            overload_test_num_messages_per_core) {
            return false;
        }
    }
    return true;
}

static bool do_ipm_overload_test(bool const disable_interrupts) {
    uint8_t const ncpus = acpi_get_number_cpus();
    num_received_msg_per_core = kmalloc(ncpus *
        sizeof(*num_received_msg_per_core));
    overload_test_ready = false;
    ipi_sent = false;

    TEST_ASSERT(ncpus > 1);

    TEST_TAG_CALLBACK = overload_test_message_callback;

    for (uint8_t cpu = 0; cpu < ncpus; ++cpu) {
        if (cpu == cpu_id()) {
            continue;
        }
        exec_remote_call(cpu,
                         overload_test_wait_and_send_messages,
                         (void*)(uint32_t)cpu_id(),
                         false);
    }

    if (disable_interrupts) {
        cpu_set_interrupt_flag(false);
    }
    overload_test_ready = true;

    if (disable_interrupts) {
        // Wait for at least one IPI to be sent to this cpu while interrupts are
        // disabled.
        bool done = false;
        while (!done) {
            lock_message_queue();
            size_t const queue_size = list_size(&this_cpu_var(message_queue));
            unlock_message_queue();
            if (queue_size) {
                break;
            } else {
                lapic_sleep(50);
            }
        }
        // We missed at least one IPI while interrupts were disabled on this
        // core. Enable them again and resume the test as normal.
        cpu_set_interrupt_flag(true);
    }

    while (!overload_test_all_messages_received()) {
        lapic_sleep(50);
    }

    kfree(num_received_msg_per_core);
    return true;
}

static bool ipm_overload_test(void) {
    return do_ipm_overload_test(false);
}

static bool ipm_while_interrupt_flag_clear_test(void) {
    return do_ipm_overload_test(true);
}


static bool *ipm_broadcast_test_recv = NULL;
static void ipm_broadcast_test_test_message_callback(
    __attribute__((unused)) struct ipm_message const * msg) {
    ipm_broadcast_test_recv[cpu_id()] = true;
}

static bool ipm_broadcast_test(void) {
    uint8_t const ncpus = acpi_get_number_cpus();
    ipm_broadcast_test_recv = kmalloc(ncpus * sizeof(*ipm_broadcast_test_recv));
    TEST_TAG_CALLBACK = ipm_broadcast_test_test_message_callback;
    
    broadcast_ipm(__TEST, NULL, 0);

    uint8_t iters = 0;
    uint8_t max_iters = 100;
    bool done = false;
    while (!done) {
        if (++iters > max_iters) {
            kfree(ipm_broadcast_test_recv);
            return false;
        }
        lapic_sleep(50);

        done = true;
        for (uint8_t cpu = 0; cpu < ncpus; ++cpu) {
            if (cpu == cpu_id()) {
                continue;
            } else if (!ipm_broadcast_test_recv[cpu]) {
                done = false;
                break;
            }
        }
    }
    kfree(ipm_broadcast_test_recv);
    return true;
}

static bool ipm_broadcast_remote_call_test(void) {
    uint8_t const ncpus = acpi_get_number_cpus();
    ipm_broadcast_test_recv = kmalloc(ncpus * sizeof(*ipm_broadcast_test_recv));

    TEST_TAG_CALLBACK = NULL;

    // Re-use the ipm_broadcast_test_test_message_callback function from the
    // ipm_broadcast_test.
    broadcast_remote_call((void*)ipm_broadcast_test_test_message_callback,
                          NULL,
                          true);

    for (uint8_t cpu = 0; cpu < ncpus; ++cpu) {
        if (cpu == cpu_id()) {
            continue;
        } 
        TEST_ASSERT(ipm_broadcast_test_recv[cpu]);
    }
    kfree(ipm_broadcast_test_recv);
    return true;
}

// IPM no deadlock test. In this test we check that remote call do not create
// deadlocks if two cores are calling functions on each other.
// The cpu running the test chooses two cores.
// Each of those cores run the function _ipm_no_deadlock_test_function.
// _ipm_no_deadlock_test_function is a loop that remotely execute
// _ipm_no_deadlock_test_signal on the other core. ipm_no_deadlock_test_rounds
// times.
// _ipm_no_deadlock_test_signal simply increment a cpu private variable.
//
// The cpu running the test waits for this variable to become == to
// ipm_no_deadlock_test_rounds on both cpus.

static uint32_t ipm_no_deadlock_test_rounds = 1024;
DECLARE_PER_CPU(uint32_t volatile, signal);
static void _ipm_no_deadlock_test_signal(void * arg) {
    this_cpu_var(signal) ++;
}

static void _ipm_no_deadlock_test_function(void * arg) {
    uint8_t const target_cpu = (uint32_t)arg;
    for (uint32_t i = 0; i < ipm_no_deadlock_test_rounds; ++i) {
        exec_remote_call(target_cpu, _ipm_no_deadlock_test_signal, NULL, true);
    }
}

#define next_cpu(cpu) ((cpu+1)%acpi_get_number_cpus())

static bool ipm_no_deadlock_test(void) {
    TEST_ASSERT(acpi_get_number_cpus() >= 3);

    uint32_t const cpu1 = next_cpu(cpu_id());
    uint32_t const cpu2 = next_cpu(cpu1);
    ASSERT(cpu_id() != cpu1);
    ASSERT(cpu_id() != cpu2);

    exec_remote_call(cpu1, _ipm_no_deadlock_test_function, (void*)cpu2, false);
    exec_remote_call(cpu2, _ipm_no_deadlock_test_function, (void*)cpu1, false);
    
    TEST_WAIT_FOR(cpu_var(signal, cpu1) != ipm_no_deadlock_test_rounds, 2000);
    TEST_WAIT_FOR(cpu_var(signal, cpu2) != ipm_no_deadlock_test_rounds, 2000);
    return true;
}

#undef next_cpu

void ipm_test(void) {
    TEST_FWK_RUN(ipm_simple_test);
    TEST_FWK_RUN(ipm_remote_call_test);
    TEST_FWK_RUN(ipm_overload_test);
    TEST_FWK_RUN(ipm_while_interrupt_flag_clear_test);
    TEST_FWK_RUN(ipm_broadcast_test);
    TEST_FWK_RUN(ipm_broadcast_remote_call_test);
    TEST_FWK_RUN(ipm_no_deadlock_test);
}
