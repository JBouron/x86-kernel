#include <test.h>
#include <ipm.h>
#include <smp.h>
#include <kmalloc.h>

// Helper function to cancel the effect of a sched_init() call. More
// specifically, this function will delete the idle processes created in
// sched_init(). Additionally, it will make sure that the curr_proc of all cpu
// is set to NULL.
static void sched_cancel_init(void) {
    for (uint16_t cpu = 0; cpu < acpi_get_number_cpus(); ++cpu) {
        struct proc * const idle = cpu_var(idle_proc, cpu);
        ASSERT(idle);
        delete_proc(idle);
        cpu_var(curr_proc, cpu) = NULL;
        cpu_var(resched_flag, cpu) = false;
        cpu_var(sched_running, cpu) = false;
        cpu_var(context_switches, cpu) = 0;
    } 
}

// ============================================================================= 
// Tests for the functions in sched_core responsible to call the correct
// callbacks. More specifically:
//  - sched_init() should call the `sched_init` callback.
//  - sched_enqueue() should call the `enqueue_proc` callback.
//  - sched_dequeue() should call the `dequeue_proc` callback.
//  - sched_update_curr() should call the `update_curr` callback.
//  - sched_tick() should call the `tick` callback.
// sched_core.c does not provide wrappers for `pick_next_proc` and
// `put_prev_proc`, those are called in schedule(). Hence those are not tested
// here but will be tested in the schedule() test.

enum callback_id {
    NONE, INIT, ENQUEUE, DEQUEUE, UPDATE_CURR, TICK, PICK_NEXT, PUT_PREV,
};

// Indicate the last callback that was called.
static enum callback_id ct_last_callback = NONE;
// Indicate the value of the struct proc pass as argument in the last callback
// if applicable.
static struct proc * ct_last_proc = NULL;

static void ct_sched_init(void) {
    ASSERT(ct_last_callback == NONE);
    ct_last_callback = INIT;
}

static void ct_enqueue_proc(struct proc * const proc) {
    ASSERT(ct_last_callback == NONE);
    ct_last_callback = ENQUEUE;
    ct_last_proc = proc;
}

static void ct_dequeue_proc(struct proc * const proc) {
    ASSERT(ct_last_callback == NONE);
    ct_last_callback = DEQUEUE;
    ct_last_proc = proc;
}

static void ct_update_curr(void) {
    ASSERT(ct_last_callback == NONE);
    ct_last_callback = UPDATE_CURR;
}

static void ct_tick(void) {
    ASSERT(ct_last_callback == NONE);
    ct_last_callback = TICK;
}

static struct sched callbacks_test_sched = {
    .sched_init       = ct_sched_init,
    .enqueue_proc     = ct_enqueue_proc,
    .dequeue_proc     = ct_dequeue_proc,
    .update_curr      = ct_update_curr,
    .tick             = ct_tick,
    .pick_next_proc   = NULL,
    .put_prev_proc    = NULL,
};

static bool sched_callbacks_test(void) {
    struct sched * const old_sched = SCHEDULER;
    SCHEDULER = &callbacks_test_sched;   

    // Create a placeholder process. It won't be executed but it needs to be
    // runnable (since we will enqueue it).
    struct proc * const proc = create_kproc(NULL, NULL);

    // Test that the callbacks are called when calling sched_core.c's wrapper
    // functions.
    sched_init();
    TEST_ASSERT(ct_last_callback == INIT);
    ct_last_callback = NONE;
    ct_last_proc = NULL;

    sched_enqueue_proc(proc);
    TEST_ASSERT(ct_last_callback == ENQUEUE);
    TEST_ASSERT(ct_last_proc == proc);
    ct_last_callback = NONE;
    ct_last_proc = NULL;

    // We need to mock the fact that the cpu is running the process.
    this_cpu_var(curr_proc) = proc;
    sched_update_curr();
    TEST_ASSERT(ct_last_callback == UPDATE_CURR);
    ct_last_callback = NONE;
    ct_last_proc = NULL;

    sched_tick(NULL);
    TEST_ASSERT(ct_last_callback == TICK);
    ct_last_callback = NONE;
    ct_last_proc = NULL;

    sched_dequeue_proc(proc);
    TEST_ASSERT(ct_last_callback == DEQUEUE);
    TEST_ASSERT(ct_last_proc == proc);
    ct_last_callback = NONE;
    ct_last_proc = NULL;

    delete_proc(proc);
    sched_cancel_init();
    SCHEDULER = old_sched;
    return true;
}

// ============================================================================= 
// curr_cpu_need_resched() test.
static bool curr_cpu_need_resched_test(void) {
    sched_init();

    // Test case 1: The cpu does not have a curr_proc. This is the first proc to
    // be executed.
    this_cpu_var(curr_proc) = NULL;
    TEST_ASSERT(curr_cpu_need_resched());

    // Test case 2: The cpu is idle, that is its current proc is its idle proc.
    this_cpu_var(curr_proc) = this_cpu_var(idle_proc);
    TEST_ASSERT(curr_cpu_need_resched());

    // Test case 3: The resched_flag is set for the current cpu.
    this_cpu_var(resched_flag) = true;
    TEST_ASSERT(curr_cpu_need_resched());
    this_cpu_var(resched_flag) = false;

    // Test case 4: The current process is not runnable.
    struct proc * const proc = create_proc();
    // Curr proc is not runnable, but is not dead either. This particular
    // scenario is not exactly realistic since no code is in the process, but
    // all we need is state_flags not being runnable and not being dead.
    this_cpu_var(curr_proc) = proc;
    TEST_ASSERT(proc->state_flags == PROC_WAITING_EIP);
    TEST_ASSERT(curr_cpu_need_resched());

    // Test case 5: The current process is dead.
    proc->state_flags = PROC_DEAD;
    TEST_ASSERT(curr_cpu_need_resched());

    // Test case 6: The current process is runnable, resched_flag is false. This
    // is the only case where a resched is not necessary.
    proc->state_flags = PROC_RUNNABLE;
    this_cpu_var(resched_flag) = false;
    TEST_ASSERT(!curr_cpu_need_resched());


    sched_cancel_init();
    delete_proc(proc);
    return true;
}

// ============================================================================= 
// schedule() test.
// This test makes sure that schedule() correctly calls the put_prev_proc
// callback on the current proc, calls the pick_next_proc to select a new proc
// and perform a context switch to that process.
// Additionally, this test makes sure that calling schedule() does not corrupt
// callee-saved registers, and respects the SYSV ABI.
//
// The test runs as follows: We have N kernel processes, all are executing the
// same code which is a "while (1) int 0x80". At every interrupt, the interrupt
// handler will set the resched_flag on the current cpu and call schedule. Hence
// we expect the following inter-leaving of processes:
//  0 1 2 3 ... N-2 N-1 0 1 2 3 ... N-2 N-1 0 1 2 3 ...
// The interrupt handler makes sure that at every interrupt the current process
// is the expected one (proc 0 for the first in, proc 1 for the second, ...).
// Additionally, the interrupt handler checks that callee-saved registers do not
// change when calling schedule() AND that context switches actually occur by
// looking at the context_switches counter of the current cpu.
// This test uses a custom scheduler in which the pick_next_proc ensure the
// above interleaving. The put_prev_proc makes sure that the prev process is the
// expected one.
// The test will run until all processes will have raised `st_num_ints_per_proc`
// interrupts, this means there will be `st_num_procs` * `st_num_ints_per_proc`
// interrupts/context switches in total.

// The number of processes to use in this test.
static uint32_t st_num_procs = 32;
// The array of all processes used in this test.
static struct proc **st_procs = NULL;
// The number of 0x80 interrupts so far.
static uint32_t st_num_ints = 0;
// The number of context switches on the current cpu before the call to
// schedule() within the interrupt handler.
static uint64_t st_last_context_switches = 0;
// The last "curr" proc before the call to schedule() within the interrupt
// handler.
static struct proc *st_last_proc = NULL;

// The maximum number of interrupt per process. This means that this test will
// stop after st_num_procs * st_num_ints_per_proc interrupts.
static uint32_t st_num_ints_per_proc = 32;
// Indicate to the cpu running this test that the test is over.
static bool volatile st_test_done = false;

// The code executed by all the processes during the test.
static void st_proc_code(void * unused) {
    while (1) {
        asm("int $0x80");
    }
}

extern void save_registers(struct register_save_area *, void*);

// The 0x80 interrupt handler.
// @param frame: unused.
static void st_int_handler(struct interrupt_frame const * const frame) {
    struct proc * const curr = this_cpu_var(curr_proc);

    st_num_ints ++;

    if (st_num_ints == st_num_procs * st_num_ints_per_proc) {
        LOG("Max number of interrupts reached. Test over and successful\n");
        st_test_done = true;
        lock_up();
    }

    st_last_context_switches = this_cpu_var(context_switches);

    // Make sure the next call to schedule() performs a context switch.
    sched_resched();

    st_last_proc = curr;

    // Right before calling schedule(), save the current value of the registers
    // onto the current stack. EIP is set to some dummy value.
    struct register_save_area regs_before_schedule;
    save_registers(&regs_before_schedule, (void*)0xBEEFBABE);

    schedule();

    // Execution resumed after a while, read the current values of the registers
    // and compare the values of the callee-save registers, they MUST be exactly
    // the same.
    struct register_save_area regs_after_schedule;
    save_registers(&regs_after_schedule, (void*)0xBEEFBABE);
    ASSERT(regs_after_schedule.edi == regs_before_schedule.edi);
    ASSERT(regs_after_schedule.esi == regs_before_schedule.esi);
    ASSERT(regs_after_schedule.ebx == regs_before_schedule.ebx);
    ASSERT(regs_after_schedule.ebp == regs_before_schedule.ebp);
    ASSERT(regs_after_schedule.esp == regs_before_schedule.esp);

    // Make sure that the last proc calling schedule() let to a context switch.
    ASSERT(this_cpu_var(context_switches) == st_last_context_switches + 1);

    // The last process MUST be the previous one in the array st_procs (wrap
    // around if curr is proc 0).
    ASSERT(st_last_proc == st_procs[(st_num_ints - 1) % st_num_procs]);

    // From our point of view, nothing should have changed.
    ASSERT(this_cpu_var(curr_proc) == curr);
    ASSERT(curr == st_procs[st_num_ints % st_num_procs]);
}

// pick_next_proc callback of the scheduler used during this test.
// This function makes sure that the interleaving of processes looks as follows:
//  0 1 2 3 ... N-2 N-1 0 1 2 ...
static struct proc *st_sched_pick_next_proc(void) {
    struct proc * const prev = this_cpu_var(curr_proc);

    // The previous process (which is the current process at this point) must be
    // the proc at index st_num_ints - 1.
    ASSERT(st_num_ints > 0);
    ASSERT(prev == st_procs[(st_num_ints - 1) % st_num_procs]);

    // The context switch did not happen yet. It will be _after_ this call.
    ASSERT(this_cpu_var(context_switches) == st_last_context_switches);

    // The next process to run is the next in the array.
    return st_procs[st_num_ints % st_num_procs];
}

// put_prev_proc callback of the scheduler used during this test. Simply checks
// that this is the expected proc.
static void st_sched_put_prev_proc(struct proc * const proc) {
    ASSERT(st_num_ints > 0);
    ASSERT(proc == st_procs[(st_num_ints - 1) % st_num_procs]);

    // The context switch did not happen yet. It will be _after_ the call to
    // pick_next_proc.
    ASSERT(this_cpu_var(context_switches) == st_last_context_switches);
}

// The scheduler used for the schedule() test. Note that since schedule() only
// uses the `pick_next_proc` and `put_prev_proc` callbacks, those are the only
// two we need.
static struct sched st_sched = {
    .sched_init       = NULL,
    .enqueue_proc     = NULL,
    .dequeue_proc     = NULL,
    .update_curr      = NULL,
    .tick             = NULL,
    .pick_next_proc   = st_sched_pick_next_proc,
    .put_prev_proc    = st_sched_put_prev_proc,
};

// Start executing the first process of st_procs.
static void st_start_exec(void * arg) {
    // Start the execution of the first process in st_procs.
    switch_to_proc(st_procs[0]);
    __UNREACHABLE__;
}

// The actual test.
static bool schedule_test(void) {
    struct sched * const old_sched = SCHEDULER;
    SCHEDULER = &st_sched;

    interrupt_register_global_callback(SYSCALL_VECTOR, st_int_handler);

    st_procs = kmalloc(st_num_procs * sizeof(*st_procs));
    st_num_ints = 0;

    for (uint32_t i = 0; i < st_num_procs; ++i) {
        st_procs[i] = create_kproc(st_proc_code, NULL);
    }

    uint8_t const cpu = TEST_TARGET_CPU(0);
    exec_remote_call(cpu, st_start_exec, NULL, false);

    while (!st_test_done) {
        lapic_sleep(50);
    }

    init_aps();

    for (uint32_t i = 0; i < st_num_procs; ++i) {
        delete_proc(st_procs[i]);
    }
    kfree(st_procs);

    interrupt_delete_global_callback(SYSCALL_VECTOR);
    SCHEDULER = old_sched;
    return true;
}

// =============================================================================
// schedule() test 2: This test will check that a cpu that is idle, or not
// running any process (curr_proc == NULL) or having its current process not
// runnable will trigger a context switch upon calling schedule().

// A runnable process. This is the process we expects the cpu to start executing
// on the next call to schedule.
static struct proc * srt_proc = NULL;

// The pick_next_proc simply return the test process srt_proc.
static struct proc *srt_sched_pick_next_proc(void) {
    return srt_proc;
}

// Since the current process is either: idle, NULL, or not runnable, we expects
// that schedule() will skip the call to put_prev_proc when doing the context
// switch to srt_proc. Hence put an ASSERT(false) to make sure this function is
// never executed.
static void srt_sched_put_prev_proc(struct proc * const proc) {
    ASSERT(false);
}

// The scheduler used for the test. Only pick_next_proc and put_prev_proc are
// required since we will interact with schedule() only.
static struct sched srt_sched = {
    .sched_init       = NULL,
    .enqueue_proc     = NULL,
    .dequeue_proc     = NULL,
    .update_curr      = NULL,
    .tick             = NULL,
    .pick_next_proc   = srt_sched_pick_next_proc,
    .put_prev_proc    = srt_sched_put_prev_proc,
};

// Call schedule on the current cpu. This function is meant to be used remotely.
static void srt_remote_schedule(void * unused) {
    schedule();
    // The call to schedule is not expected to return, this is because the cpu
    // will be reset between each test case. This assert also makes sure that a
    // context switch happened in the call to schedule().
    ASSERT(false);
}

// Start the test case remotely. This function will set the curr_proc to the
// proc passed as argument (even if NULL or not-runnable) and start execution if
// it is runnable.
// @param arg: A struct proc * to use as curr_proc.
static void srt_remote_start(void * arg) {
    struct proc * const proc = arg;

    if (proc && proc_is_runnable(proc)) {
        ASSERT(proc == this_cpu_var(idle_proc));
        switch_to_proc(arg);
    } else {
        LOG("Proc not runnable\n");
        this_cpu_var(curr_proc) = proc;
    }
}

// Indicate if srt_proc is done with its execution.
static bool volatile srt_proc_done = false;

// The code executed by srt_proc. Simply set srt_proc_done to true and lock_up
// the cpu.
static void srt_proc_code(void * arg) {
    srt_proc_done = true;

    // Wait to be reset by the cpu running the test.
    while (1) {
        cpu_set_interrupt_flag_and_halt();
    }
}

static bool schedule_resched_test(void) {
    sched_init();
    struct sched * const old_sched = SCHEDULER;
    SCHEDULER = &srt_sched;

    uint8_t const cpu = TEST_TARGET_CPU(0);
    for (uint8_t i = 0; i < 3; ++i) {
        if (i) {
            init_aps();
            delete_proc(srt_proc);
        }

        struct proc * remote_curr = NULL;
        switch (i) {
            // Test case 1: The current process is NULL.
            case 0:
                remote_curr = NULL;
                break;

            // Test case 2: The current process is idle_proc, the cpu is
            // considered idle.
            case 1:
                remote_curr = cpu_var(idle_proc, cpu);
                break;

            // Test case 3: The current process is not runnable anymore.
            case 2:
                remote_curr = create_proc();
                break;
        }

        // Set the curr_proc on the remote cpu and start execution (only
        // applicable for the idle case).
        exec_remote_call(cpu, srt_remote_start, remote_curr, false);

        // Create a new runnable process.
        srt_proc_done = false;
        srt_proc = create_kproc(srt_proc_code, NULL);

        // Call schedule() on the remote cpu.
        exec_remote_call(cpu, srt_remote_schedule, NULL, false);

        // Since the remote cpu is either idle, not running any process or has
        // its current process non-runnable, the call to schedule() is expected
        // to start executing srt_proc.
        TEST_WAIT_FOR(srt_proc_done, 100);

        if (i == 2) {
            delete_proc(remote_curr);
        }
    }
    init_aps();
    sched_cancel_init();
    delete_proc(srt_proc);
    SCHEDULER = old_sched;
    return true;
}

// =============================================================================
// sched_update_curr() test. This test makes sure that sched_update_curr()
// either call the update_curr callback of the scheduler if curr is runnable or
// set the resched_flag if it is not.

// Indicate if the update_curr callback has been called.
static bool volatile suct_update_curr_called = false;
static void suct_update_curr(void) {
    suct_update_curr_called = true;
}

// The scheduler used by the test. Only update_curr is required here.
static struct sched suct_sched = {
    .update_curr = suct_update_curr,
};

static bool sched_update_curr_test(void) {
    struct sched * const old_sched = SCHEDULER;
    SCHEDULER = &suct_sched;

    // Test case 1: The current proc is runnable. sched_update_curr() is
    // expected to call the update_curr callback. No rescheduling is requested.
    struct proc * const proc = create_kproc(NULL, NULL);
    this_cpu_var(curr_proc) = proc;
    suct_update_curr_called = false;
    sched_update_curr();
    TEST_ASSERT(suct_update_curr_called);
    TEST_ASSERT(!this_cpu_var(resched_flag));

    // Test case 2: The current process is not runnable, sched_update_curr()
    // should not call update_curr and instead should set the resched_flag.
    proc->state_flags = PROC_WAITING_EIP;
    this_cpu_var(curr_proc) = proc;
    this_cpu_var(resched_flag) = false;

    // Case 2.1: state_flags == PROC_WAITING_EIP.
    suct_update_curr_called = false;
    sched_update_curr();
    TEST_ASSERT(!suct_update_curr_called);
    TEST_ASSERT(this_cpu_var(resched_flag));
    this_cpu_var(resched_flag) = false;

    // Case 2.2: state_flags == PROC_DEAD.
    proc->state_flags = PROC_DEAD;
    suct_update_curr_called = false;
    sched_update_curr();
    TEST_ASSERT(!suct_update_curr_called);
    TEST_ASSERT(this_cpu_var(resched_flag));
    this_cpu_var(resched_flag) = false;

    SCHEDULER = old_sched;
    delete_proc(proc);
    this_cpu_var(resched_flag) = false;
    this_cpu_var(curr_proc) = NULL;
    return true;
}

void sched_test(void) {
    TEST_FWK_RUN(sched_callbacks_test);
    TEST_FWK_RUN(curr_cpu_need_resched_test);
    TEST_FWK_RUN(schedule_test);
    TEST_FWK_RUN(schedule_resched_test);
    TEST_FWK_RUN(sched_update_curr_test);
}
