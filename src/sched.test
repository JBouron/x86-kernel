#include <test.h>
#include <ipm.h>
#include <smp.h>
#include <kmalloc.h>
#include <list.h>

// Helper function to cancel the effect of a sched_init() call. More
// specifically, this function will delete the idle processes created in
// sched_init(). Additionally, it will make sure that the curr_proc of all cpu
// is set to NULL.
static void sched_cancel_init(void) {
    for (uint16_t cpu = 0; cpu < acpi_get_number_cpus(); ++cpu) {
        struct proc * const idle = cpu_var(idle_proc, cpu);
        ASSERT(idle);
        delete_proc(idle);
        cpu_var(curr_proc, cpu) = NULL;
        cpu_var(resched_flag, cpu) = false;
        cpu_var(sched_running, cpu) = false;
        cpu_var(context_switches, cpu) = 0;
    } 
}

// ============================================================================= 
// Tests for the functions in sched_core responsible to call the correct
// callbacks. More specifically:
//  - sched_init() should call the `sched_init` callback.
//  - sched_enqueue() should call the `enqueue_proc` callback.
//  - sched_dequeue() should call the `dequeue_proc` callback.
//  - sched_update_curr() should call the `update_curr` callback.
//  - sched_tick() should call the `tick` callback.
// sched_core.c does not provide wrappers for `pick_next_proc` and
// `put_prev_proc`, those are called in schedule(). Hence those are not tested
// here but will be tested in the schedule() test.

enum callback_id {
    NONE, INIT, ENQUEUE, DEQUEUE, UPDATE_CURR, TICK, PICK_NEXT, PUT_PREV,
};

// Indicate the last callback that was called.
static enum callback_id ct_last_callback = NONE;
// Indicate the value of the struct proc pass as argument in the last callback
// if applicable.
static struct proc * ct_last_proc = NULL;

static void ct_sched_init(void) {
    ASSERT(ct_last_callback == NONE);
    ct_last_callback = INIT;
}

static void ct_enqueue_proc(struct proc * const proc) {
    ASSERT(ct_last_callback == NONE);
    ct_last_callback = ENQUEUE;
    ct_last_proc = proc;
}

static void ct_dequeue_proc(struct proc * const proc) {
    ASSERT(ct_last_callback == NONE);
    ct_last_callback = DEQUEUE;
    ct_last_proc = proc;
}

static void ct_update_curr(void) {
    ASSERT(ct_last_callback == NONE);
    ct_last_callback = UPDATE_CURR;
}

static void ct_tick(void) {
    ASSERT(ct_last_callback == NONE);
    ct_last_callback = TICK;
}

static struct sched callbacks_test_sched = {
    .sched_init       = ct_sched_init,
    .enqueue_proc     = ct_enqueue_proc,
    .dequeue_proc     = ct_dequeue_proc,
    .update_curr      = ct_update_curr,
    .tick             = ct_tick,
    .pick_next_proc   = NULL,
    .put_prev_proc    = NULL,
};

static bool sched_callbacks_test(void) {
    struct sched * const old_sched = SCHEDULER;
    SCHEDULER = &callbacks_test_sched;   

    // Create a placeholder process. It won't be executed but it needs to be
    // runnable (since we will enqueue it).
    struct proc * const proc = create_kproc(NULL, NULL);

    // Test that the callbacks are called when calling sched_core.c's wrapper
    // functions.
    sched_init();
    TEST_ASSERT(ct_last_callback == INIT);
    ct_last_callback = NONE;
    ct_last_proc = NULL;

    sched_enqueue_proc(proc);
    TEST_ASSERT(ct_last_callback == ENQUEUE);
    TEST_ASSERT(ct_last_proc == proc);
    ct_last_callback = NONE;
    ct_last_proc = NULL;

    // We need to mock the fact that the cpu is running the process.
    this_cpu_var(curr_proc) = proc;
    sched_update_curr();
    TEST_ASSERT(ct_last_callback == UPDATE_CURR);
    ct_last_callback = NONE;
    ct_last_proc = NULL;

    sched_tick(NULL);
    TEST_ASSERT(ct_last_callback == TICK);
    ct_last_callback = NONE;
    ct_last_proc = NULL;

    sched_dequeue_proc(proc);
    TEST_ASSERT(ct_last_callback == DEQUEUE);
    TEST_ASSERT(ct_last_proc == proc);
    ct_last_callback = NONE;
    ct_last_proc = NULL;

    delete_proc(proc);
    sched_cancel_init();
    SCHEDULER = old_sched;
    return true;
}

// ============================================================================= 
// curr_cpu_need_resched() test.
static bool curr_cpu_need_resched_test(void) {
    sched_init();

    // Test case 1: The cpu does not have a curr_proc. This is the first proc to
    // be executed.
    this_cpu_var(curr_proc) = NULL;
    TEST_ASSERT(curr_cpu_need_resched());

    // Test case 2: The cpu is idle, that is its current proc is its idle proc.
    this_cpu_var(curr_proc) = this_cpu_var(idle_proc);
    TEST_ASSERT(curr_cpu_need_resched());

    // Test case 3: The resched_flag is set for the current cpu.
    this_cpu_var(resched_flag) = true;
    TEST_ASSERT(curr_cpu_need_resched());
    this_cpu_var(resched_flag) = false;

    // Test case 4: The current process is not runnable.
    struct proc * const proc = create_proc();
    // Curr proc is not runnable, but is not dead either. This particular
    // scenario is not exactly realistic since no code is in the process, but
    // all we need is state_flags not being runnable and not being dead.
    this_cpu_var(curr_proc) = proc;
    TEST_ASSERT(proc->state_flags == PROC_WAITING_EIP);
    TEST_ASSERT(curr_cpu_need_resched());

    // Test case 5: The current process is dead.
    proc->state_flags = PROC_DEAD;
    TEST_ASSERT(curr_cpu_need_resched());

    // Test case 6: The current process is runnable, resched_flag is false. This
    // is the only case where a resched is not necessary.
    proc->state_flags = PROC_RUNNABLE;
    this_cpu_var(resched_flag) = false;
    TEST_ASSERT(!curr_cpu_need_resched());


    sched_cancel_init();
    delete_proc(proc);
    return true;
}

// ============================================================================= 
// schedule() test.
// This test makes sure that schedule() correctly calls the put_prev_proc
// callback on the current proc, calls the pick_next_proc to select a new proc
// and perform a context switch to that process.
// Additionally, this test makes sure that calling schedule() does not corrupt
// callee-saved registers, and respects the SYSV ABI.
//
// The test runs as follows: We have N kernel processes, all are executing the
// same code which is a "while (1) int 0x80". At every interrupt, the interrupt
// handler will set the resched_flag on the current cpu and call schedule. Hence
// we expect the following inter-leaving of processes:
//  0 1 2 3 ... N-2 N-1 0 1 2 3 ... N-2 N-1 0 1 2 3 ...
// The interrupt handler makes sure that at every interrupt the current process
// is the expected one (proc 0 for the first in, proc 1 for the second, ...).
// Additionally, the interrupt handler checks that callee-saved registers do not
// change when calling schedule() AND that context switches actually occur by
// looking at the context_switches counter of the current cpu.
// This test uses a custom scheduler in which the pick_next_proc ensure the
// above interleaving. The put_prev_proc makes sure that the prev process is the
// expected one.
// The test will run until all processes will have raised `st_num_ints_per_proc`
// interrupts, this means there will be `st_num_procs` * `st_num_ints_per_proc`
// interrupts/context switches in total.

// The number of processes to use in this test.
static uint32_t st_num_procs = 32;
// The array of all processes used in this test.
static struct proc **st_procs = NULL;
// The number of 0x80 interrupts so far.
static uint32_t st_num_ints = 0;
// The number of context switches on the current cpu before the call to
// schedule() within the interrupt handler.
static uint64_t st_last_context_switches = 0;
// The last "curr" proc before the call to schedule() within the interrupt
// handler.
static struct proc *st_last_proc = NULL;

// The maximum number of interrupt per process. This means that this test will
// stop after st_num_procs * st_num_ints_per_proc interrupts.
static uint32_t st_num_ints_per_proc = 32;
// Indicate to the cpu running this test that the test is over.
static bool volatile st_test_done = false;

// The code executed by all the processes during the test.
static void st_proc_code(void * unused) {
    while (1) {
        asm("int $0x80");
    }
}

extern void save_registers(struct register_save_area *, void*);

// The 0x80 interrupt handler.
// @param frame: unused.
static void st_int_handler(struct interrupt_frame const * const frame) {
    struct proc * const curr = this_cpu_var(curr_proc);

    st_num_ints ++;

    if (st_num_ints == st_num_procs * st_num_ints_per_proc) {
        LOG("Max number of interrupts reached. Test over and successful\n");
        st_test_done = true;
        lock_up();
    }

    st_last_context_switches = this_cpu_var(context_switches);

    // Make sure the next call to schedule() performs a context switch.
    sched_resched();

    st_last_proc = curr;

    // Right before calling schedule(), save the current value of the registers
    // onto the current stack. EIP is set to some dummy value.
    struct register_save_area regs_before_schedule;
    save_registers(&regs_before_schedule, (void*)0xBEEFBABE);

    schedule();

    // Execution resumed after a while, read the current values of the registers
    // and compare the values of the callee-save registers, they MUST be exactly
    // the same.
    struct register_save_area regs_after_schedule;
    save_registers(&regs_after_schedule, (void*)0xBEEFBABE);
    ASSERT(regs_after_schedule.edi == regs_before_schedule.edi);
    ASSERT(regs_after_schedule.esi == regs_before_schedule.esi);
    ASSERT(regs_after_schedule.ebx == regs_before_schedule.ebx);
    ASSERT(regs_after_schedule.ebp == regs_before_schedule.ebp);
    ASSERT(regs_after_schedule.esp == regs_before_schedule.esp);

    // Make sure that the last proc calling schedule() let to a context switch.
    ASSERT(this_cpu_var(context_switches) == st_last_context_switches + 1);

    // The last process MUST be the previous one in the array st_procs (wrap
    // around if curr is proc 0).
    ASSERT(st_last_proc == st_procs[(st_num_ints - 1) % st_num_procs]);

    // From our point of view, nothing should have changed.
    ASSERT(this_cpu_var(curr_proc) == curr);
    ASSERT(curr == st_procs[st_num_ints % st_num_procs]);
}

// pick_next_proc callback of the scheduler used during this test.
// This function makes sure that the interleaving of processes looks as follows:
//  0 1 2 3 ... N-2 N-1 0 1 2 ...
static struct proc *st_sched_pick_next_proc(void) {
    struct proc * const prev = this_cpu_var(curr_proc);

    // The previous process (which is the current process at this point) must be
    // the proc at index st_num_ints - 1.
    ASSERT(st_num_ints > 0);
    ASSERT(prev == st_procs[(st_num_ints - 1) % st_num_procs]);

    // The context switch did not happen yet. It will be _after_ this call.
    ASSERT(this_cpu_var(context_switches) == st_last_context_switches);

    // The next process to run is the next in the array.
    return st_procs[st_num_ints % st_num_procs];
}

// put_prev_proc callback of the scheduler used during this test. Simply checks
// that this is the expected proc.
static void st_sched_put_prev_proc(struct proc * const proc) {
    ASSERT(st_num_ints > 0);
    ASSERT(proc == st_procs[(st_num_ints - 1) % st_num_procs]);

    // The put_prev_proc happens after the context_switches is updated.
    ASSERT(this_cpu_var(context_switches) == st_last_context_switches + 1);
}

// The scheduler used for the schedule() test. Note that since schedule() only
// uses the `pick_next_proc` and `put_prev_proc` callbacks, those are the only
// two we need.
static struct sched st_sched = {
    .sched_init       = NULL,
    .enqueue_proc     = NULL,
    .dequeue_proc     = NULL,
    .update_curr      = NULL,
    .tick             = NULL,
    .pick_next_proc   = st_sched_pick_next_proc,
    .put_prev_proc    = st_sched_put_prev_proc,
};

// Start executing the first process of st_procs.
static void st_start_exec(void * arg) {
    // Start the execution of the first process in st_procs.
    switch_to_proc(st_procs[0]);
    __UNREACHABLE__;
}

// The actual test.
static bool schedule_test(void) {
    struct sched * const old_sched = SCHEDULER;
    SCHEDULER = &st_sched;

    interrupt_register_global_callback(SYSCALL_VECTOR, st_int_handler);

    st_procs = kmalloc(st_num_procs * sizeof(*st_procs));
    st_num_ints = 0;

    for (uint32_t i = 0; i < st_num_procs; ++i) {
        st_procs[i] = create_kproc(st_proc_code, NULL);
    }

    uint8_t const cpu = TEST_TARGET_CPU(0);
    exec_remote_call(cpu, st_start_exec, NULL, false);

    while (!st_test_done) {
        lapic_sleep(50);
    }

    init_aps();

    for (uint32_t i = 0; i < st_num_procs; ++i) {
        delete_proc(st_procs[i]);
    }
    kfree(st_procs);

    interrupt_delete_global_callback(SYSCALL_VECTOR);
    SCHEDULER = old_sched;
    return true;
}

// =============================================================================
// schedule() test 2: This test will check that a cpu that is idle, or not
// running any process (curr_proc == NULL) or having its current process not
// runnable (or dead) will trigger a context switch upon calling schedule().

// A runnable process. This is the process we expects the cpu to start executing
// on the next call to schedule.
static struct proc * srt_proc = NULL;

// The pick_next_proc simply return the test process srt_proc.
static struct proc *srt_sched_pick_next_proc(void) {
    return srt_proc;
}

// Since the current process is either: idle, NULL, or not runnable, we expects
// that schedule() will skip the call to put_prev_proc when doing the context
// switch to srt_proc. Hence put an ASSERT(false) to make sure this function is
// never executed.
static void srt_sched_put_prev_proc(struct proc * const proc) {
    ASSERT(false);
}

// The scheduler used for the test. Only pick_next_proc and put_prev_proc are
// required since we will interact with schedule() only.
static struct sched srt_sched = {
    .sched_init       = NULL,
    .enqueue_proc     = NULL,
    .dequeue_proc     = NULL,
    .update_curr      = NULL,
    .tick             = NULL,
    .pick_next_proc   = srt_sched_pick_next_proc,
    .put_prev_proc    = srt_sched_put_prev_proc,
};

// Call schedule on the current cpu. This function is meant to be used remotely.
static void srt_remote_schedule(void * unused) {
    schedule();
    // The call to schedule is not expected to return, this is because the cpu
    // will be reset between each test case. This assert also makes sure that a
    // context switch happened in the call to schedule().
    ASSERT(false);
}

// Start the test case remotely. This function will set the curr_proc to the
// proc passed as argument (even if NULL or not-runnable) and start execution if
// it is runnable.
// @param arg: A struct proc * to use as curr_proc.
static void srt_remote_start(void * arg) {
    cpu_set_interrupt_flag(false);
    struct proc * const proc = arg;

    this_cpu_var(curr_proc) = NULL;

    if (proc && proc_is_runnable(proc)) {
        ASSERT(proc == this_cpu_var(idle_proc));
        switch_to_proc(arg);
    } else {
        LOG("Proc not runnable\n");
        this_cpu_var(curr_proc) = proc;
    }
}

// Indicate if srt_proc is done with its execution.
static bool volatile srt_proc_done = false;

// The code executed by srt_proc. Simply set srt_proc_done to true and lock_up
// the cpu.
static void srt_proc_code(void * arg) {
    srt_proc_done = true;

    // Wait to be reset by the cpu running the test.
    while (1) {
        cpu_set_interrupt_flag_and_halt();
    }
}

static bool schedule_resched_test(void) {
    sched_init();
    struct sched * const old_sched = SCHEDULER;
    SCHEDULER = &srt_sched;

    uint8_t const cpu = TEST_TARGET_CPU(0);
    for (uint8_t i = 0; i < 4; ++i) {
        if (i) {
            init_aps();
            delete_proc(srt_proc);
        }

        struct proc * remote_curr = NULL;
        switch (i) {
            // Test case 1: The current process is NULL.
            case 0:
                remote_curr = NULL;
                break;

            // Test case 2: The current process is idle_proc, the cpu is
            // considered idle.
            case 1:
                remote_curr = cpu_var(idle_proc, cpu);
                break;

            // Test case 3: The current process is not runnable anymore.
            case 2:
                remote_curr = create_proc();
                break;

            // Test case 4: The current process is dead.
            case 3:
                remote_curr = create_proc();
                remote_curr->state_flags = PROC_DEAD;
                break;
        }

        // Set the curr_proc on the remote cpu and start execution (only
        // applicable for the idle case).
        exec_remote_call(cpu, srt_remote_start, remote_curr, false);

        // Create a new runnable process.
        srt_proc_done = false;
        srt_proc = create_kproc(srt_proc_code, NULL);

        // Call schedule() on the remote cpu.
        exec_remote_call(cpu, srt_remote_schedule, NULL, false);

        // Since the remote cpu is either idle, not running any process or has
        // its current process non-runnable, the call to schedule() is expected
        // to start executing srt_proc.
        TEST_WAIT_FOR(srt_proc_done, 100);

        if (i == 2 || i == 3) {
            delete_proc(remote_curr);
        }
    }
    init_aps();
    sched_cancel_init();
    delete_proc(srt_proc);
    SCHEDULER = old_sched;
    return true;
}

// =============================================================================
// sched_update_curr() test. This test makes sure that sched_update_curr()
// either call the update_curr callback of the scheduler if curr is runnable or
// set the resched_flag if it is not.

// Indicate if the update_curr callback has been called.
static bool volatile suct_update_curr_called = false;
static void suct_update_curr(void) {
    suct_update_curr_called = true;
}

// The scheduler used by the test. Only update_curr is required here.
static struct sched suct_sched = {
    .update_curr = suct_update_curr,
};

static bool sched_update_curr_test(void) {
    struct sched * const old_sched = SCHEDULER;
    SCHEDULER = &suct_sched;

    // Test case 1: The current proc is runnable. sched_update_curr() is
    // expected to call the update_curr callback. No rescheduling is requested.
    struct proc * const proc = create_kproc(NULL, NULL);
    this_cpu_var(curr_proc) = proc;
    suct_update_curr_called = false;
    sched_update_curr();
    TEST_ASSERT(suct_update_curr_called);
    TEST_ASSERT(!this_cpu_var(resched_flag));

    // Test case 2: The current process is not runnable, sched_update_curr()
    // should not call update_curr and instead should set the resched_flag.
    proc->state_flags = PROC_WAITING_EIP;
    this_cpu_var(curr_proc) = proc;
    this_cpu_var(resched_flag) = false;

    // Case 2.1: state_flags == PROC_WAITING_EIP.
    suct_update_curr_called = false;
    sched_update_curr();
    TEST_ASSERT(!suct_update_curr_called);
    TEST_ASSERT(this_cpu_var(resched_flag));
    this_cpu_var(resched_flag) = false;

    // Case 2.2: state_flags == PROC_DEAD.
    proc->state_flags = PROC_DEAD;
    suct_update_curr_called = false;
    sched_update_curr();
    TEST_ASSERT(!suct_update_curr_called);
    TEST_ASSERT(this_cpu_var(resched_flag));
    this_cpu_var(resched_flag) = false;

    SCHEDULER = old_sched;
    delete_proc(proc);
    this_cpu_var(resched_flag) = false;
    this_cpu_var(curr_proc) = NULL;
    return true;
}

// =============================================================================
// schedule() migration stress test. In this test we have a single process
// shared between all APs. At all times the APs call schedule() and try to run
// the process. Only one of them will succeed.
// This test makes sure that even though the process can quickly go from
// executing on a core and being migrated to another, no stack or state
// corruption occurs. This is a regression test for a previous bug where a cpu
// would put_prev_proc(curr) but still use curr's stack afterwards leading to
// corruption if another cpu would start executing curr at the same time.
//
// The test process will be a simple infinite loop calling sched_resched()
// followed by schedule(). The idle processes (private to each cpu) will be the
// same so that a cpu is always trying to schedule a new proc (in this case the
// test proc).
//
// The test will run until the test process has migrated `sst_num_migrations`
// times.

// The test process.
static struct proc * sst_proc = NULL;

// Code of the test process. Note that this is also the code of the idle process
// of each core.
static void sst_proc_code(void * unused) {
    while (true) {
        sched_resched();
        schedule();
    }
}

// Lock protecting the test process.
static DECLARE_SPINLOCK(sst_proc_lock);
// Only one cpu can run the test process at a time. This flag indicate if the
// process is running (false) or free to be run on a cpu (true).
static bool volatile sst_proc_free = true;
// The number of migrations of the test process so far.
static uint32_t sst_num_migrations = 0;
// The maximum number of migrations of the test process / terminating condition.
static uint32_t sst_max_num_migrations = 2048;
// Notify the cpu running the test that a cpu is done running the test and did
// not experience errors.
DECLARE_PER_CPU(bool, sst_success) = false;

// The pick_next_proc callback of the scheduler used in this test. Since we only
// have one process of interest (the test process), this function will return
// the test process if it is free, NO_PROC otherwise.
static struct proc *sst_pick_next_proc(void) {
    spinlock_lock(&sst_proc_lock);

    struct proc * next = NULL;
    if (sst_proc_free) {
        // Update the migration count.
        if (sst_proc->cpu != cpu_id()) {
            sst_num_migrations++;
        }

        // If we reach the max number of migration, lock up and wait for the
        // reset.
        if (sst_num_migrations >= sst_max_num_migrations) {
            LOG("[%u] Done\n", cpu_id());
            this_cpu_var(sst_success) = true;
            // Let other cpus in.
            spinlock_unlock(&sst_proc_lock);
            lock_up();
        } else {
            // The current cpu won the race and got the right to execute the
            // test process.
            sst_proc_free = false;
            next = sst_proc;
        }
    } else {
        // Test process is already running on another cpu.
        next = NO_PROC;
    }

    spinlock_unlock(&sst_proc_lock);
    return next;
}

// The put_prev_proc callback of the test scheduler. Simply marks the test
// process as free.
static void sst_put_prev(struct proc * const prev) {
    ASSERT(prev == sst_proc);
    ASSERT(!sst_proc_free);
    sst_proc_free = true;
}

// The scheduler used in this test.
static struct sched sst_sched = {
    .pick_next_proc = sst_pick_next_proc,
    .put_prev_proc = sst_put_prev,
};

// The actual test.
static bool schedule_stress_test(void) {
    struct sched * const old_sched = SCHEDULER;
    SCHEDULER = &sst_sched;

    // Create the special idle processes for the remote cores. The idle proc is
    // identical to the test proc, it is calling schedule as much as possible in
    // a loop.
    uint16_t const ncpus = acpi_get_number_cpus();
    for (uint16_t cpu = 0; cpu < ncpus; ++cpu) {
        struct proc * const idle = create_kproc(sst_proc_code, NULL);
        cpu_var(idle_proc, cpu) = idle;
        cpu_var(sst_success, cpu) = false;
    }

    // Create the test process.
    sst_proc = create_kproc(sst_proc_code, NULL);

    // Start the execution on all cores simulteanously.
    broadcast_remote_call((void*)schedule, NULL, false);

    // Wait for all the migrations to occur.
    TEST_WAIT_FOR(sst_num_migrations >= sst_max_num_migrations, 5000);
    LOG("All %u migrations occured\n", sst_max_num_migrations);

    // Make sure that all cpus are ok and completed the test successfully.
    for (uint16_t cpu = 0; cpu < ncpus; ++cpu) {
        if (cpu == cpu_id()) {
            continue;
        }
        TEST_WAIT_FOR(cpu_var(sst_success, cpu), 1000);
    }
    LOG("All APs OK, test successful\n");

    // Test complete. Reset the APs.
    init_aps();

    // Clean up.
    delete_proc(sst_proc);
    for (uint16_t cpu = 0; cpu < ncpus; ++cpu) {
        delete_proc(cpu_var(idle_proc, cpu));
        cpu_var(idle_proc, cpu) = NULL;
    }

    SCHEDULER = old_sched;
    return true;
}


// =============================================================================
// preemption test. This test ensures that when disabling preemption, the
// current process of a cpu will not be preempted no matter what happens
// (interrupts, ticks, explicit calls to schedule(), ...).
//
// This test is composed of multiple test kernel processes all running the same
// code pt_proc_code. This code will simulate multiple test cases multiple times
// before signaling to the cpu running the test that it is done.
// The test runs with scheduling ticks enabled to generate as much interrupts as
// possible. Each tick will also enforce a round of scheduling by setting the
// resched_flags and calling schedule().

// Indicates to the cpu running the test how many processes are done executing
// and successfully passed all the checks/test cases.
static atomic_t pt_procs_done;
// The number of scheduler ticks during this test per cpu.
DECLARE_PER_CPU(uint32_t, pt_num_ticks) = 0;

// This is the code executed by all the processes used in this test. This code
// simulates multiple test cases related ot preemption.
static void pt_proc_code(void * unused) {
    // Get a pointer on self, this will be used to kill ourselves at the end of
    // the test.
    preempt_disable();
    struct proc * const self = this_cpu_var(curr_proc);
    preempt_enable();

    uint32_t const max_ite = 32;
    for (uint32_t i = 0; i < max_ite; ++i) {
        uint8_t cpu_before, cpu_after;
        uint64_t cs_before, cs_after;

        // Test case 1: We are calling scheduler (with the resched_flags
        // enabled) while having preemption enabled. This should result in no
        // context switch and schedule should return immediately.
        ASSERT(preemptible());
        preempt_disable();
        ASSERT(!preemptible());

        cpu_before = cpu_id();
        cs_before = this_cpu_var(context_switches);

        // Sanity check that the percpu variables are correct.
        ASSERT(cpu_before == this_cpu_var(curr_proc)->cpu);
        ASSERT(cpu_before == cpu_apic_id());

        // Try to resched.
        sched_resched();
        schedule();

        cs_after = this_cpu_var(context_switches);
        cpu_after = cpu_id();

        preempt_enable();
        ASSERT(preemptible());

        // The number of context switches should be the same as before, that
        // means that no context switch happened in the last call to schedule.
        ASSERT(cs_before == cs_after);

        // Since there were no context switch, no migration was possible.
        ASSERT(cpu_before == cpu_after);


        // Test case 2: We are doing a long operations with preemption disabled.
        // This operation lasts for multiple ticks. Once again no context
        // switches nor migrations is expected.
        preempt_disable();
        ASSERT(!preemptible());

        cpu_before = cpu_id();
        cs_before = this_cpu_var(context_switches);
        uint32_t const tick_start = this_cpu_var(pt_num_ticks);
        // Wait for at least 10 ticks.
        while (this_cpu_var(pt_num_ticks) < tick_start + 10) {
            ASSERT(cpu_id() == cpu_before);
            ASSERT(this_cpu_var(context_switches) == cs_before);
        }

        preempt_enable();
        ASSERT(preemptible());


        // Test case 3:  Preemption is only enabled after the number of calls to
        // preempt_enable() is the same as the number of calls to
        // preempt_disable().
        ASSERT(preemptible());
        preempt_disable();
        ASSERT(!preemptible());
        preempt_disable();
        ASSERT(!preemptible());

        preempt_enable();
        ASSERT(!preemptible());

        cpu_before = cpu_id();
        cs_before = this_cpu_var(context_switches);

        // Sanity check that the percpu variables are correct.
        ASSERT(cpu_before == this_cpu_var(curr_proc)->cpu);
        ASSERT(cpu_before == cpu_apic_id());

        // Try to resched. Not possible because the cpu is still
        // not-preemptible.
        sched_resched();
        schedule();

        cs_after = this_cpu_var(context_switches);
        cpu_after = cpu_id();

        // Finally re-enable the preemption for next test case.
        preempt_enable();
        ASSERT(preemptible());


        // Test case 4: Sanity check, do the same as above without disabling
        // preemption and check that the process eventually gets migrated.
        cpu_before = cpu_apic_id();
        while (cpu_id() == cpu_before) {
            cpu_pause();
        }
    }

    // This process managed to pass all the cases multiple times. It is now
    // done.
    atomic_inc(&pt_procs_done);

    // Declare this process as dead so that it won't run after the call to
    // schedule() below.
    self->state_flags = PROC_DEAD;
    schedule();

    __UNREACHABLE__;
}

// The total number of processes to use for this test.
static uint32_t pt_num_procs = 16;
// All the processes used in this test.
static struct proc **pt_procs = NULL;
// The queue of processes used by the custom scheduler for this test.
static struct list_node pt_queue;
// Protect the queue of processes.
DECLARE_SPINLOCK(pt_procs_lock);

// pick_next_proc callback for the custom scheduler used in this test.
// @return: The first process in the `pt_queue`. If the queue is empty, return
// NO_PROC instead.
static struct proc *pt_pick_next_proc(void) {
    struct proc * next = NULL;

    spinlock_lock(&pt_procs_lock);
    if (!list_size(&pt_queue)) {
        // No more processes to run.
        next = NO_PROC;
    } else {
        next = list_first_entry(&pt_queue, struct proc, rq);
        list_del(&next->rq);
    }
    spinlock_unlock(&pt_procs_lock);

    return next;
}

// put_prev_proc callback for the custom scheduler.
// @param proc: The process to enqueue back into the scheduler.
static void pt_put_prev_proc(struct proc * const proc) {
    spinlock_lock(&pt_procs_lock);
    list_add_tail(&pt_queue, &proc->rq);
    spinlock_unlock(&pt_procs_lock);
}

// tick callback of the custom scheduler. This function will increment the
// pt_num_ticks variable of the current cpu, set the resched flag and call
// schedule().
static void pt_tick(void) {
    preempt_disable();
    this_cpu_var(pt_num_ticks) ++;
    preempt_enable();

    sched_resched();
    schedule();
}

// The custom scheduler used by this test.
static struct sched pt_sched = {
    .tick = pt_tick,
    .pick_next_proc = pt_pick_next_proc,
    .put_prev_proc = pt_put_prev_proc,
};

// The actual test.
static bool preemption_test(void) {
    // One of the test cases is for the process to be migrated to another cpu.
    // Since a cpu is running the test we need at least 3 cores in order to see
    // migrations.
    TEST_ASSERT(acpi_get_number_cpus() > 2);

    // Initialize the idle processes.
    sched_init();

    struct sched * const old_sched = SCHEDULER;
    SCHEDULER = &pt_sched;

    atomic_init(&pt_procs_done, 0);
    list_init(&pt_queue);
    pt_procs = kmalloc(pt_num_procs * sizeof(*pt_procs));

    // Create the test processes and enqueue them into pt_queue. Note: No lock
    // required here, since only one cpu is running.
    for (uint32_t i = 0; i < pt_num_procs; ++i) {
        pt_procs[i] = create_kproc(pt_proc_code, NULL);
        list_add_tail(&pt_queue, &pt_procs[i]->rq);
    }

    // Start the scheduler on remote cpus.
    broadcast_remote_call((void*)sched_start, NULL, false);

    // Wait for all the processes to succeed.
    TEST_WAIT_FOR((uint32_t)atomic_read(&pt_procs_done) == pt_num_procs, 10000);

    // We need to stop the lapic timer on remote cpus before resetting them.
    // Otherwise they could receive an unexpected timer interrupt after the
    // reset.
    broadcast_remote_call((void*)lapic_stop_timer, NULL, true);

    // Reset and clean up.
    init_aps();
    for (uint32_t i = 0; i < pt_num_procs; ++i) {
        delete_proc(pt_procs[i]);
    }
    kfree(pt_procs);

    SCHEDULER = old_sched;
    sched_cancel_init();
    return true;
}

void sched_test(void) {
    TEST_FWK_RUN(sched_callbacks_test);
    TEST_FWK_RUN(curr_cpu_need_resched_test);
    TEST_FWK_RUN(schedule_test);
    TEST_FWK_RUN(schedule_resched_test);
    TEST_FWK_RUN(sched_update_curr_test);
    TEST_FWK_RUN(schedule_stress_test);
    TEST_FWK_RUN(preemption_test);
}
