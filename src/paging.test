#include <test.h>
#include <interrupt.h>

// Test the result of create_recursive_entry.
static bool paging_create_recursive_entry_test(void) {
    struct page_dir_t dir;
    memzero(&dir, sizeof(dir));

    // We use the virtual address for the dir here, but that is fine.
    create_recursive_entry(&dir);

    // All entries except the last one should still be 0.
    for (uint16_t i = 0; i < 1023; ++i) {
        TEST_ASSERT(!dir.entry[i].val);
    }

    union pde_t const last = dir.entry[1023];
    TEST_ASSERT(last.present == 1);
    TEST_ASSERT(last.writable == 1);
    TEST_ASSERT(last.user_accessible == 0);
    TEST_ASSERT(last.write_through == 1);
    TEST_ASSERT(last.cache_disable == 1);
    TEST_ASSERT(last.accessed == 0);
    TEST_ASSERT(last.page_table_addr == ((uint32_t)&dir) >> 12);
    return true;
}

static bool paging_get_curr_page_dir_vaddr_test(void) {
    uint32_t const vaddr = (uint32_t)get_curr_page_dir_vaddr();
    return vaddr == 0xFFFFF000;
}

static bool paging_get_page_table_vaddr_test() {
    uint16_t const index = 0x1AB;
    uint32_t const vaddr = (uint32_t)get_page_table_vaddr(index);
    TEST_ASSERT(vaddr >> 22 == 1023);
    TEST_ASSERT(((vaddr >> 12) & 0x3FF) == index);
    return true;
}

static bool paging_pde_and_pte_index_test(void) {
    uint32_t const pde_idx = 0x123;
    uint32_t const pte_idx = 0x1BA;
    void const * const addr = (void*)((pde_idx << 22) | (pte_idx << 12) | 0x74);
    return pde_index(addr) == pde_idx && pte_index(addr) == pte_idx;
}

// Call map_page and check that the PDE and PTE have been mutated accordingly.
static bool _paging_map_page_page_table_test_runner(uint32_t const flags) {
    void const * const paddr = (void*)0xD00000; 
    void const * const vaddr = (void*)0xF00000; 

    uint32_t const pde_idx = pde_index(vaddr);
    uint32_t const pte_idx = pte_index(vaddr);

    map_page(get_curr_page_dir_vaddr(), paddr, vaddr, flags);

    union pde_t const pde = get_curr_page_dir_vaddr()->entry[pde_idx];
    union pte_t const pte = get_page_table_vaddr(pde_idx)->entry[pte_idx];

    unmap_page(vaddr, false);
    cpu_invalidate_tlb();


    TEST_ASSERT(pde.present == 1);
    TEST_ASSERT(pde.writable == 1);
    TEST_ASSERT(pde.user_accessible == 0);
    TEST_ASSERT(pde.write_through == 0);
    TEST_ASSERT(pde.cache_disable == 0);
    TEST_ASSERT(pde.accessed == 1);

    TEST_ASSERT(pte.present == 1);
    TEST_ASSERT(pte.writable == (bool)(flags & VM_WRITE));
    TEST_ASSERT(pte.user_accessible == (bool)(flags & VM_USER));
    TEST_ASSERT(pte.write_through == (bool)(flags & VM_WRITE_THROUGH));
    TEST_ASSERT(pte.cache_disable == (bool)(flags & VM_CACHE_DISABLE));
    TEST_ASSERT(pte.accessed == 0);
    TEST_ASSERT(pte.dirty == 0);
    TEST_ASSERT(pte.zero == 0);
    TEST_ASSERT(pte.global == (bool)!(flags & VM_NON_GLOBAL));
    TEST_ASSERT(pte.frame_addr == ((uint32_t)paddr) >> 12);

    return true;
}
static bool paging_map_page_page_table_test(void) {
    // Try out all combinations of flags.
    for (uint32_t flag = 0; flag <= 0b11111; ++flag) {
        TEST_ASSERT(_paging_map_page_page_table_test_runner(flag));
    }
    return true;
}

static bool paging_map_test(void) {
    void const * const paddr = to_phys(KERNEL_START_ADDR);
    uint8_t const * const vaddr = (uint8_t*)0xF00000; 

    // Map a couple of pages in read only.
    paging_map(paddr, vaddr, 4 * PAGE_SIZE, 0);

    // Check that we read the same thing.
    uint8_t const * const higher_half = (uint8_t*)KERNEL_START_ADDR;
    bool success = true;
    for (size_t i = 0; i < 4 * PAGE_SIZE; ++i) {
        if (*(higher_half + i) != *(vaddr + i)) {
            success = false;
            break;
        }
    }

    paging_unmap(vaddr, 4 * PAGE_SIZE);

    return success;
}

// End to end test for paging / page-fault / interrupt.
// The goal here is to trigger page fault and make sure that:
//  1) We detect it.
//  2) The correct CR2 is reported.
//  3) The correct error code is reported.
//  4) We can resolve them mapping the faulting memory.
// To spice things up we trigger two page faults:
//  1) A write to a page that is not marked present.
//  2) Another writing a page that is read only.
//
// The test setup is as follows:
//  1) Register a handler for the interrupt 14 (PF).
//  2) Trigger a page fault by writing to a virtual memory address that is not
//  mapped.
//  3) In the handler: Record the values of CR2 and error code, map the faulting
//  virtual address to a new physical frame (freshly allocate) in _read only_.
//  4) Return from the handler, the write resumes execution but triggers another
//  segfault as the page is read-only.
//  5) In the handler: Set the writable bit for the physical frame mapped to the
//  faulting address.
//  6) Return from the handler. The write should now succeed.
//  7) After the write check the values of the CR2 and error code for the two
//  page fault. There should be as follow:
//      _ CR2 should be the same in both fault, that is the address used to
//      perform the write.
//      _ Error code should be 2 in the first fault as it is a ring 0 write on a
//      non-present page.
//      _ Error code should be 3 in the second fault as it is a ring 0 write on
//      a present page that triggered a protection fault.
//  8) Finally reading the value from the same pointer should return the value
//  we wrote.

// The number of page fault handled by the dummy handler below.
static volatile uint8_t number_page_fault = 0;
// The faulting address, per fault that happened.
static volatile void const* fault_addrs[2];
// The error code, per fault that happened.
static volatile uint32_t error_codes[2];

// The temporary page fault handler.
static void dummy_pf_handler(struct interrupt_frame_t const * const frame) {
    void const * const cr2 = cpu_read_cr2();

    // Save the faulting addr and error code for this fault.
    fault_addrs[number_page_fault] = cr2;
    error_codes[number_page_fault] = frame->error_code;

    number_page_fault ++;

    void const * const page_addr = get_page_addr(cr2);
    if (number_page_fault == 1) {
        // On the first page fault, map the page read only. This will trigger
        // a second segfault, at this point map it with write permissions.
        void const * const phy_frame = alloc_frame();
        paging_map(phy_frame, page_addr, PAGE_SIZE, 0);
    } else {
        // Enable write bit on the second fault.
        uint32_t const pde_idx = pde_index(cr2);
        uint32_t const pte_idx = pte_index(cr2);
        struct page_table_t * const page_table = get_page_table_vaddr(pde_idx);
        page_table->entry[pte_idx].writable = 1;
        cpu_invalidate_tlb();
    }
}

static bool paging_pagefault_test(void) {
    // Register the dummy handler and enable interrupts.
    interrupt_register_callback(14, dummy_pf_handler);
    cpu_set_interrupt_flag(true);

    // Use the following address to trigger a page fault.
    void const * const fault_addr = (void*)0xBEEFDEAD;

    // The write will trigger two page faults as describe in the comment above.
    *((uint32_t*)fault_addr) = 0xCAFEBABE;

    // Read the memory addr that was faulting before. We will later use it to
    // make sure the write went-through.
    uint32_t const read_val = *((uint32_t*)fault_addr);

    // Unmap the page mapped by the dummy_pf_handler function.
    void const * const page_addr = get_page_addr(fault_addr);
    paging_unmap(page_addr, PAGE_SIZE);

    // Both faults should have been triggered by the faulting address above.
    TEST_ASSERT(fault_addrs[0] == fault_addr);
    TEST_ASSERT(fault_addrs[1] == fault_addr);
    // First error was a write to a page that was not present.
    TEST_ASSERT(error_codes[0] == 2);
    // Second error is a write to a non-writable page.
    TEST_ASSERT(error_codes[1] == 3);
    // We should have experienced 2 faults only.
    TEST_ASSERT(number_page_fault == 2);
    // The write must have went through.
    TEST_ASSERT(read_val == 0xCAFEBABE);

    cpu_set_interrupt_flag(false);
    interrupt_delete_callback(14);
    return true;
}

// Test unmapping a page.
static bool paging_unmap_page_test(void) {
    void const * const frame = alloc_frame();
    void const * const vaddr = (void*)0xBEEFDEAD;

    // Map the virtual addr to a physical frame.
    paging_map(frame, get_page_addr(vaddr), PAGE_SIZE, 0);

    // Reset the attributes of the page fault handler.
    fault_addrs[0] = NULL;
    fault_addrs[1] = NULL;
    error_codes[0] = 0;
    error_codes[1] = 0;
    number_page_fault = 0;

    interrupt_register_callback(14, dummy_pf_handler);
    cpu_set_interrupt_flag(true);

    // Since the virtual address is mapped we should be able to read it without
    // trigerring a page fault.
    uint32_t const __attribute__((unused)) read1 = *((uint32_t*)vaddr);
    TEST_ASSERT(number_page_fault == 0);

    // Unmap the page.
    paging_unmap(get_page_addr(vaddr), PAGE_SIZE);

    // Re-read the pointer again. This time we expect a page fault. Note that
    // the handler will map the address again, but not necessarily to the same
    // physical frame. This does not really matter for this test as were are not
    // interested in the values read.
    uint32_t const __attribute__((unused)) read2 = *((uint32_t volatile*)vaddr);

    TEST_ASSERT(number_page_fault == 1);
    TEST_ASSERT(fault_addrs[0] == vaddr);
    // Privilege read on a non-present page => error code 0
    TEST_ASSERT(error_codes[0] == 0);

    // Since the page fault handler had to map the virtual page we need to unmap
    // it again.
    paging_unmap(get_page_addr(vaddr), PAGE_SIZE);

    cpu_set_interrupt_flag(false);
    interrupt_delete_callback(14);
    return true;
}

// Tests that when unmapping a virtual address, the paging_unmap function will
// free any frames that contains a page table that is not required anymore.
static bool paging_unmap_frees_frames_test(void) {
    // The number of pages allocated at the start of this test.
    uint32_t const allocated_at_start = frames_allocated();

    // Map a huge memory area, the goal here is to force paging_map to allocate
    // as many frames for new page tables as possible.
    void const * const paddr = (void*)NULL;
    void const * const vaddr = (void*)NULL;
    // 16MiB map.
    size_t const len = 16 * (1 << 20);

    // The flags don't matter at all.
    paging_map(paddr, vaddr, len, 0);

    // Compute the number of new frames allocated by paging_map. Each allocated
    // frame should correspond to a new page table.
    uint32_t const delta_allocs = frames_allocated() - allocated_at_start;

    // Unmap.
    paging_unmap(vaddr, len);
    // We expect one page table per 4MiB mapped, hence the same amount for
    // allocation.
    uint32_t const expected_delta = ceil_x_over_y_u32(len, 4 << 20);
    return expected_delta == delta_allocs &&
        frames_allocated() == allocated_at_start;
}

// Test for page_is_mapped.
static bool paging_page_is_mapped_test(void) {
    void const * const frame = alloc_frame();

    TEST_ASSERT(!page_is_mapped(frame));
    paging_map(frame, frame, PAGE_SIZE, 0);
    TEST_ASSERT(page_is_mapped(frame));

    paging_unmap(frame, PAGE_SIZE);
    TEST_ASSERT(!page_is_mapped(frame));
    free_frame(frame);
    return true;
}

static bool paging_find_next_non_mapped_page_test(void) {
    void * const frame = alloc_frame();

    TEST_ASSERT(find_next_non_mapped_page(frame) == frame);

    // Now map some pages starting at address frame. Map 1, 2, ..., 8 pages and
    // check that find_next_non_mapped_page return the correct address.
    for (uint8_t i = 0; i < 8; ++i) {
        paging_map(frame, frame + i * PAGE_SIZE, PAGE_SIZE, 0);
        // This is the address of the page rigth after the one we mapped above.
        // This page should be non mapped.
        void * const exp_addr = frame + (i+1) * PAGE_SIZE;
        TEST_ASSERT(find_next_non_mapped_page(frame) == exp_addr);
    }

    // Cleanup.
    for (uint8_t i = 0; i < 8; ++i) {
        paging_unmap(frame + i * PAGE_SIZE, PAGE_SIZE);
    }

    free_frame(frame);
    return true;
}

static bool paging_compute_hole_size_test(void) {
    void * const frame = alloc_frame();

    // If nothing is mapped in the lower half then we expect the hole starting
    // at address 0x0 to span all the way to the start of the kernel.
    uint32_t const num_frames_to_kernel = (uint32_t)KERNEL_PHY_OFFSET >> 12;
    TEST_ASSERT(compute_hole_size(NULL) == num_frames_to_kernel);

    // map the frame to its identity postion + at 3 pages after that.
    // essentially we are creating a hole between the two mapped pages of size 2
    // * page_size.
    paging_map(frame, frame, PAGE_SIZE, 0);
    paging_map(frame, frame + 3 * PAGE_SIZE, PAGE_SIZE, 0);

    TEST_ASSERT(compute_hole_size(0x0) == (uint32_t)frame >> 12);

    TEST_ASSERT(!compute_hole_size(frame));
    TEST_ASSERT(compute_hole_size(frame + PAGE_SIZE) == 2);

    paging_map(frame, frame + 2 * PAGE_SIZE, PAGE_SIZE, 0);
    TEST_ASSERT(compute_hole_size(frame + PAGE_SIZE) == 1);

    paging_unmap(frame, PAGE_SIZE);
    paging_unmap(frame + 2 * PAGE_SIZE, PAGE_SIZE);
    paging_unmap(frame + 3 * PAGE_SIZE, PAGE_SIZE);
    free_frame(frame);
    return true;
}

// Test the paging_find_contiguous_non_mapped_pages.
static bool paging_find_contiguous_non_mapped_pages_test(void) {
    void * const frame = alloc_frame();

    // Map the frame to its identity postion + at 3 pages after that.
    // Essentially we are creating a hole between the two mapped pages of size 2
    // * PAGE_SIZE.
    paging_map(frame, frame, PAGE_SIZE, 0);
    paging_map(frame, frame + 3 * PAGE_SIZE, PAGE_SIZE, 0);

    void * const start2 = paging_find_contiguous_non_mapped_pages(frame, 2);
    TEST_ASSERT(start2 == frame + PAGE_SIZE);

    // We cannot fit 3 frames between the two pages, it has to go after the
    // second page.
    void * const start3 = paging_find_contiguous_non_mapped_pages(frame, 3);
    TEST_ASSERT(start3 == frame + 4 * PAGE_SIZE);

    paging_unmap(frame, PAGE_SIZE);
    paging_unmap(frame + 3 * PAGE_SIZE, PAGE_SIZE);
    free_frame(frame);
    return true;
}

void paging_test(void) {
    TEST_FWK_RUN(paging_create_recursive_entry_test);
    TEST_FWK_RUN(paging_get_curr_page_dir_vaddr_test);
    TEST_FWK_RUN(paging_get_page_table_vaddr_test);
    TEST_FWK_RUN(paging_pde_and_pte_index_test);
    TEST_FWK_RUN(paging_map_page_page_table_test);
    TEST_FWK_RUN(paging_map_test);
    TEST_FWK_RUN(paging_pagefault_test);
    TEST_FWK_RUN(paging_unmap_page_test);
    TEST_FWK_RUN(paging_unmap_frees_frames_test);
    TEST_FWK_RUN(paging_page_is_mapped_test);
    TEST_FWK_RUN(paging_find_next_non_mapped_page_test);
    TEST_FWK_RUN(paging_compute_hole_size_test);
    TEST_FWK_RUN(paging_find_contiguous_non_mapped_pages_test);
}
